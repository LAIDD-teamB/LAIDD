# LAIDD - Learning-based AI Drug Discovery

ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹ ì•½ ê°œë°œ í†µí•© íŒŒì´í”„ë¼ì¸

## ğŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ì„¤ì¹˜](#ì„¤ì¹˜)
- [ë°ì´í„° ì¤€ë¹„](#ë°ì´í„°-ì¤€ë¹„)
- [ì‚¬ìš©ë²•](#ì‚¬ìš©ë²•)
- [í”„ë¡œì íŠ¸ êµ¬ì¡°](#í”„ë¡œì íŠ¸-êµ¬ì¡°)
- [ê° ëª¨ë“ˆ ì„¤ëª…](#ê°-ëª¨ë“ˆ-ì„¤ëª…)

## ğŸ¯ ê°œìš”

LAIDDëŠ” ë‹¤ìŒ 3ë‹¨ê³„ë¡œ êµ¬ì„±ëœ ì‹ ì•½ ê°œë°œ íŒŒì´í”„ë¼ì¸ì…ë‹ˆë‹¤:

1. **Pretraining**: BindingDB ë°ì´í„°ë¥¼ ì´ìš©í•œ Multi-Affinity í•™ìŠµ
2. **Fine-tuning**: GSK3Î² íƒ€ê²Ÿ íŠ¹í™” ëª¨ë¸ í•™ìŠµ
3. **Generative Model**: LSTM ê¸°ë°˜ ì‹ ê·œ ë¶„ì ìƒì„±

## ğŸ”§ ì„¤ì¹˜

### í•„ìˆ˜ ìš”êµ¬ì‚¬í•­

- Python 3.8 ì´ìƒ
- CUDA (GPU ì‚¬ìš© ì‹œ)

### ì˜ì¡´ì„± íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
# RDKit ì„¤ì¹˜
pip install rdkit-pypi

# PyTorch ì„¤ì¹˜ (CUDA ë²„ì „ì— ë§ê²Œ)
pip install torch torchvision torchaudio

# ê¸°íƒ€ íŒ¨í‚¤ì§€
pip install pandas numpy tqdm matplotlib scikit-learn
```

## ğŸ“ ë°ì´í„° ì¤€ë¹„

í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— `data` í´ë”ë¥¼ ìƒì„±í•˜ê³  ë‹¤ìŒ íŒŒì¼ë“¤ì„ ë°°ì¹˜í•˜ì„¸ìš”:

```
LAIDD/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ generative_data.csv      # ìƒì„± ëª¨ë¸ í•™ìŠµ ë°ì´í„°
â”‚   â”œâ”€â”€ pretraining_data.tsv     # Pretraining ë°ì´í„° (BindingDB)
â”‚   â””â”€â”€ finetuning_data.tsv      # Fine-tuning ë°ì´í„° (GSK3Î²)
```

### ë°ì´í„° íŒŒì¼ í˜•ì‹

**generative_data.csv**:
- ì»¬ëŸ¼: `SMILES` ë˜ëŠ” `smiles`, `ID` ë˜ëŠ” `Molecule_ID`

**pretraining_data.tsv**:
- í•„ìˆ˜ ì»¬ëŸ¼: `Standardized_SMILES`, `Standard Type`, `pChEMBL_Value`, `Target_Sequence`
- ì§€ì›í•˜ëŠ” affinity íƒ€ì…: IC50, Ki, EC50, Kd

**finetuning_data.tsv**:
- í•„ìˆ˜ ì»¬ëŸ¼: `Ligand SMILES`, `BindingDB Target Chain Sequence`, `Standard Type`, `pChEMBL`

## ğŸš€ ì‚¬ìš©ë²•

### ë°©ë²• 1: í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (ê¶Œì¥)

```bash
# ë°ì´í„° íŒŒì¼ í™•ì¸
python run_all.py --check-only

# ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
python run_all.py --all

# ê°œë³„ ë‹¨ê³„ ì‹¤í–‰
python run_all.py --pretraining    # Pretrainingë§Œ
python run_all.py --finetuning     # Fine-tuningë§Œ
python run_all.py --generative     # Generative modelë§Œ

# ê¸°ì¡´ ê²°ê³¼ ë¬´ì‹œí•˜ê³  ì¬ì‹¤í–‰
python run_all.py --all --no-skip
```

### ë°©ë²• 2: ê°œë³„ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰

```bash
# 1. Pretraining
python pretraining_code.py

# 2. Fine-tuning
python finetuning_code.py

# 3. Generative Model
python generativemodel.py
```

### ì»¤ìŠ¤í…€ ê²½ë¡œ ì§€ì •

```bash
# Generative modelì— ì»¤ìŠ¤í…€ ë°ì´í„° ì‚¬ìš©
python generativemodel.py --data /path/to/data.csv --output /path/to/output

# ê°œë³„ ì‹¤í–‰ ì‹œ ê²½ë¡œëŠ” ì½”ë“œ ë‚´ì—ì„œ ìˆ˜ì • ê°€ëŠ¥
```

## ğŸ“‚ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
LAIDD/
â”œâ”€â”€ README.md                    # ì´ ë¬¸ì„œ
â”œâ”€â”€ run_all.py                   # í†µí•© ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ pretraining_code.py          # Pretraining ëª¨ë“ˆ
â”œâ”€â”€ finetuning_code.py           # Fine-tuning ëª¨ë“ˆ
â”œâ”€â”€ generativemodel.py           # Generative model ëª¨ë“ˆ
â”‚
â”œâ”€â”€ bs_denovo/                   # BioGen ë¼ì´ë¸ŒëŸ¬ë¦¬
â”‚   â”œâ”€â”€ vocab.py
â”‚   â”œâ”€â”€ lang_data.py
â”‚   â”œâ”€â”€ lang_lstm.py
â”‚   â””â”€â”€ gen_eval.py
â”‚
â”œâ”€â”€ data/                        # ë°ì´í„° í´ë” (ì‚¬ìš©ìê°€ ì¤€ë¹„)
â”‚   â”œâ”€â”€ generative_data.csv
â”‚   â”œâ”€â”€ pretraining_data.tsv
â”‚   â””â”€â”€ finetuning_data.tsv
â”‚
â””â”€â”€ results/                     # ì‹¤í–‰ ê²°ê³¼ (ìë™ ìƒì„±)
    â”œâ”€â”€ pretraining/
    â”‚   â”œâ”€â”€ best_model.pt
    â”‚   â”œâ”€â”€ train_split.tsv
    â”‚   â”œâ”€â”€ val_split.tsv
    â”‚   â”œâ”€â”€ test_split.tsv
    â”‚   â””â”€â”€ final_results.json
    â”‚
    â”œâ”€â”€ finetuning/
    â”‚   â”œâ”€â”€ best_gsk3b_model.pt
    â”‚   â”œâ”€â”€ train_split.tsv
    â”‚   â”œâ”€â”€ val_split.tsv
    â”‚   â”œâ”€â”€ test_split.tsv
    â”‚   â””â”€â”€ gsk3b_finetuning_results.json
    â”‚
    â””â”€â”€ generativemodel/
        â”œâ”€â”€ models/
        â”‚   â””â”€â”€ lstm_e*.ckpt
        â”œâ”€â”€ generated_molecules.csv
        â”œâ”€â”€ generation_metrics.json
        â”œâ”€â”€ molecules_split.csv
        â”œâ”€â”€ my_tokens.txt
        â””â”€â”€ training_curves.png
```

## ğŸ” ê° ëª¨ë“ˆ ì„¤ëª…

### 1. Pretraining (pretraining_code.py)

**ëª©ì **: BindingDB ë°ì´í„°ë¥¼ ì‚¬ìš©í•œ Multi-Affinity í•™ìŠµ

**ì£¼ìš” ê¸°ëŠ¥**:
- 4ê°€ì§€ affinity íƒ€ì… ë™ì‹œ í•™ìŠµ (IC50, Ki, EC50, Kd)
- Scaffold-based ë°ì´í„° ë¶„í• 
- Multi-task learningì„ í†µí•œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ

**ì¶œë ¥**:
- `results/pretraining/best_model.pt`: ìµœê³  ì„±ëŠ¥ ëª¨ë¸
- `results/pretraining/final_results.json`: í•™ìŠµ ê²°ê³¼ ë©”íŠ¸ë¦­
- ë°ì´í„° ë¶„í•  íŒŒì¼ (train/val/test)

**ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
- Batch size: 32
- Learning rate: 1e-3
- Epochs: 30
- Model: Conv-based encoder + Multi-head predictor

### 2. Fine-tuning (finetuning_code.py)

**ëª©ì **: GSK3Î² íƒ€ê²Ÿì— íŠ¹í™”ëœ ëª¨ë¸ í•™ìŠµ

**ì£¼ìš” ê¸°ëŠ¥**:
- Pretrained ëª¨ë¸ ê¸°ë°˜ ì „ì´ í•™ìŠµ
- ë°ì´í„° ì¦ê°•ì„ í†µí•œ ê³¼ì í•© ë°©ì§€
- Single model ë˜ëŠ” 5-fold cross-validation ì§€ì›

**ì¶œë ¥**:
- `results/finetuning/best_gsk3b_model.pt`: ìµœê³  ì„±ëŠ¥ ëª¨ë¸
- `results/finetuning/gsk3b_finetuning_results.json`: í‰ê°€ ê²°ê³¼
- ë°ì´í„° ë¶„í•  íŒŒì¼

**ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
- Batch size: 16 (ì‘ì€ ë°ì´í„°ì…‹)
- Learning rate: 1e-4 (ë‚®ì€ í•™ìŠµë¥ )
- Epochs: 50
- Early stopping patience: 10

### 3. Generative Model (generativemodel.py)

**ëª©ì **: LSTM ê¸°ë°˜ ì‹ ê·œ ë¶„ì ìƒì„±

**ì£¼ìš” ê¸°ëŠ¥**:
- SMILES ê¸°ë°˜ ë¶„ì ìƒì„±
- Scaffold-based ë°ì´í„° ë¶„í• 
- ìƒì„± ë¶„ìì˜ validity, uniqueness, novelty í‰ê°€

**ì¶œë ¥**:
- `results/generativemodel/generated_molecules.csv`: ìƒì„±ëœ ë¶„ì (20,000ê°œ)
- `results/generativemodel/generation_metrics.json`: ìƒì„± í’ˆì§ˆ ë©”íŠ¸ë¦­
- `results/generativemodel/training_curves.png`: í•™ìŠµ ê³¡ì„ 
- `results/generativemodel/models/`: í•™ìŠµëœ ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸

**ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°**:
- LSTM hidden units: [512, 512, 512] (3-layer)
- Embedding size: 256
- Batch size: 256
- Max epochs: 80
- Target loss: 22.0

**í‰ê°€ ë©”íŠ¸ë¦­**:
- **Validity**: ìƒì„±ëœ SMILESê°€ ìœ íš¨í•œ ë¶„ìì¸ì§€
- **Uniqueness**: ì¤‘ë³µ ì—†ì´ ë‹¤ì–‘í•œ ë¶„ì ìƒì„±
- **Novelty**: í•™ìŠµ ë°ì´í„°ì— ì—†ëŠ” ìƒˆë¡œìš´ ë¶„ì
- **Internal Diversity**: ìƒì„±ëœ ë¶„ì ê°„ êµ¬ì¡°ì  ë‹¤ì–‘ì„±

## ğŸ“Š ê²°ê³¼ í™•ì¸

### Pretraining ê²°ê³¼

```python
import json

with open('results/pretraining/final_results.json', 'r') as f:
    results = json.load(f)

print("Test RMSE:", results['test_overall']['RMSE'])
print("Test RÂ²:", results['test_overall']['R2'])
```

### Fine-tuning ê²°ê³¼

```python
with open('results/finetuning/gsk3b_finetuning_results.json', 'r') as f:
    results = json.load(f)

print("Test RMSE:", results['test_metrics']['RMSE'])
print("Test RÂ²:", results['test_metrics']['R2'])
```

### Generative Model ê²°ê³¼

```python
import pandas as pd

# ìƒì„±ëœ ë¶„ì ë¡œë“œ
df = pd.read_csv('results/generativemodel/generated_molecules.csv')

# ìœ íš¨í•œ ë¶„ìë§Œ í•„í„°ë§
valid_df = df[df['valid'] == 1]
print(f"Valid molecules: {len(valid_df)} / {len(df)}")

# ë©”íŠ¸ë¦­ í™•ì¸
with open('results/generativemodel/generation_metrics.json', 'r') as f:
    metrics = json.load(f)

print("Validity:", metrics['validity_overall'])
print("Uniqueness:", metrics['uniqueness'])
print("Novelty:", metrics['novelty'])
```

## ğŸ› ï¸ íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. Import ì˜¤ë¥˜

```python
ModuleNotFoundError: No module named 'bs_denovo'
```

**í•´ê²°**: `bs_denovo` í´ë”ê°€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— ìˆëŠ”ì§€ í™•ì¸

### 2. CUDA out of memory

**í•´ê²°**: Batch sizeë¥¼ ì¤„ì´ê±°ë‚˜ GPU ë©”ëª¨ë¦¬ê°€ í° í™˜ê²½ì—ì„œ ì‹¤í–‰

```python
# pretraining_code.py ë˜ëŠ” finetuning_code.pyì—ì„œ
batch_size=16  # 32ì—ì„œ 16ìœ¼ë¡œ ê°ì†Œ
```

### 3. RDKit ì„¤ì¹˜ ì˜¤ë¥˜

**í•´ê²°**: conda í™˜ê²½ ì‚¬ìš© ê¶Œì¥

```bash
conda install -c conda-forge rdkit
```

### 4. ë°ì´í„° íŒŒì¼ í˜•ì‹ ì˜¤ë¥˜

**í•´ê²°**:
- TSV íŒŒì¼ì€ íƒ­ìœ¼ë¡œ êµ¬ë¶„ë˜ì–´ì•¼ í•¨
- í•„ìˆ˜ ì»¬ëŸ¼ëª…ì´ ì •í™•í•œì§€ í™•ì¸
- ì¸ì½”ë”©ì€ UTF-8 ì‚¬ìš©

## ğŸ“ ì»¤ìŠ¤í„°ë§ˆì´ì§•

### í•˜ì´í¼íŒŒë¼ë¯¸í„° ìˆ˜ì •

ê° ìŠ¤í¬ë¦½íŠ¸ì˜ `main()` ë˜ëŠ” `if __name__ == "__main__":` ì„¹ì…˜ì—ì„œ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# pretraining_code.py
model, results = main_multi_affinity_bindingdb(
    tsv_path=TSV_PATH,
    batch_size=64,      # ìˆ˜ì • ê°€ëŠ¥
    epochs=50,          # ìˆ˜ì • ê°€ëŠ¥
    lr=5e-4,            # ìˆ˜ì • ê°€ëŠ¥
    ...
)
```

### ìƒˆë¡œìš´ affinity íƒ€ì… ì¶”ê°€

```python
# pretraining_code.py
AFFINITY_TYPES = ['IC50', 'Ki', 'EC50', 'Kd', 'NEW_TYPE']  # ìƒˆë¡œìš´ íƒ€ì… ì¶”ê°€
```

### ëª¨ë¸ ì•„í‚¤í…ì²˜ ìˆ˜ì •

```python
# pretraining_code.pyì˜ MultiAffinityNet í´ë˜ìŠ¤
model = MultiAffinityNet(
    d_model=512,        # 256ì—ì„œ 512ë¡œ ì¦ê°€
    n_layers=3,         # ë ˆì´ì–´ ìˆ˜ ì¦ê°€
    dropout=0.2         # Dropout ë¹„ìœ¨ ì¡°ì •
)
```

## ğŸ“ˆ ì„±ëŠ¥ ìµœì í™” íŒ

1. **GPU í™œìš©**: CUDA ì‚¬ìš© ì‹œ í•™ìŠµ ì†ë„ 10ë°° ì´ìƒ í–¥ìƒ
2. **Batch size ì¡°ì •**: GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ìµœëŒ€í•œ í¬ê²Œ ì„¤ì •
3. **Gradient accumulation**: ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì‚¬ìš©
4. **Early stopping**: ë¶ˆí•„ìš”í•œ í•™ìŠµ ì‹œê°„ ì ˆì•½
5. **Mixed precision training**: AMP ì‚¬ìš©ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½

## ğŸ¤ ê¸°ì—¬

ë²„ê·¸ ë¦¬í¬íŠ¸ë‚˜ ê¸°ëŠ¥ ì œì•ˆì€ ì´ìŠˆë¡œ ë“±ë¡í•´ì£¼ì„¸ìš”.

## ğŸ“„ ë¼ì´ì„¼ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” êµìœ¡ ë° ì—°êµ¬ ëª©ì ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤.

## ğŸ‘¥ ê°œë°œì

LAIDD Team

---

**ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸**: 2025-10-17
