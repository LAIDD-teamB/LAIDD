##### GSK3Î² Fine-tuning Code for Multi-Affinity Model (Fixed)
# ===========================
# PyTorch ë²„ì „ í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
# ReduceLROnPlateau verbose ë§¤ê°œë³€ìˆ˜ ì œê±°
# ===========================

import os, re, math, random, subprocess, sys
import pandas as pd
import numpy as np
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
import json
from copy import deepcopy

# ---------------------------
# ê¸°ì¡´ ëª¨ë¸ êµ¬ì¡° import (pretraining_code.pyì—ì„œ)
# ---------------------------
sys.path.insert(0, os.path.join(os.path.dirname(os.path.dirname(__file__)), 'pretraining'))

from pretraining_code import (
    MultiAffinityNet, MultiAffinityDataset, collate_fn_multi,
    smiles_vocab, aa_vocab, AFFINITY_TYPES, affinity_to_id,
    compute_metrics, compute_metrics_by_affinity, EarlyStopper,
    run_epoch_multi_amp, SEED
)

# ---------------------------
# GSK3Î² Fine-tuning Dataset Class
# ---------------------------
class GSK3BDataset(MultiAffinityDataset):
    """
    GSK3Î² íŠ¹í™” ë°ì´í„°ì…‹ í´ë˜ìŠ¤
    ì‹¤ìŠµë…¸íŠ¸ì˜ ì ‘ê·¼ë°©ì‹ì„ ë”°ë¼ ì‘ì€ ë°ì´í„°ì…‹ì— ëŒ€í•œ ê³¼ì í•© ë°©ì§€ ê³ ë ¤
    """
    def __init__(self, df, max_len_smiles=256, max_len_seq=1500, 
                 augment_data=True, noise_level=0.01):
        # ê¸°ë³¸ ì´ˆê¸°í™”
        super().__init__(df, max_len_smiles, max_len_seq, drop_missing=True)
        self.augment_data = augment_data
        self.noise_level = noise_level
        
        # GSK3Î² íŠ¹í™” ì •ë³´ ì¶œë ¥
        print(f"[GSK3B INFO] íŒŒì¸íŠœë‹ ë°ì´í„°: {len(self.df)}ê°œ")
        print(f"[GSK3B INFO] ë°ì´í„° ì¦ê°•: {augment_data}")
        print(f"[GSK3B INFO] Target Sequence ê¸¸ì´: {self.df['Target_Sequence'].str.len().mean():.0f} Â± {self.df['Target_Sequence'].str.len().std():.0f}")
        
    def __getitem__(self, idx):
        # ê¸°ë³¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        item = super().__getitem__(idx)
        
        # ì‹¤ìŠµë…¸íŠ¸: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ë°ì´í„° ì¦ê°•
        if self.augment_data and random.random() < 0.3:  # 30% í™•ë¥ ë¡œ ë…¸ì´ì¦ˆ ì¶”ê°€
            # pValueì— ì‘ì€ ë…¸ì´ì¦ˆ ì¶”ê°€ (ì‹¤ì œ ì‹¤í—˜ ì˜¤ì°¨ ëª¨ì‚¬)
            noise = torch.normal(0, self.noise_level, size=(1,)).item()
            item['y'] = torch.clamp(item['y'] + noise, 0, 15)  # pIC50 ë²”ìœ„ ì œí•œ
            
        return item

# ---------------------------
# Fine-tuning ì „ìš© í•™ìŠµ í•¨ìˆ˜
# ---------------------------
def fine_tune_gsk3b(pretrained_model_path,
                    gsk3b_data_path,
                    output_dir="./finetuning_results",
                    epochs=50,
                    lr=1e-4,  # ì‹¤ìŠµë…¸íŠ¸: íŒŒì¸íŠœë‹ì—ì„œëŠ” ë‚®ì€ í•™ìŠµë¥  ì‚¬ìš©
                    weight_decay=1e-4,
                    batch_size=16,  # ì‘ì€ ë°ì´í„°ì…‹ì´ë¯€ë¡œ ì‘ì€ ë°°ì¹˜ í¬ê¸°
                    early_stopping_patience=10,
                    validation_split=0.2,
                    test_split=0.1,
                    freeze_encoder=False,  # ì¸ì½”ë” ë™ê²° ì—¬ë¶€
                    data_augmentation=True):
    
    print("=" * 80)
    print("GSK3Î² Fine-tuning for Multi-Affinity Prediction")
    print("=" * 80)
    print(f"Pre-trained model: {pretrained_model_path}")
    print(f"GSK3Î² data: {gsk3b_data_path}")
    print(f"Output directory: {output_dir}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(output_dir, exist_ok=True)
    
    # ---------------------------
    # 1. ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬
    # ---------------------------
    print("\n[STEP 1] ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬")
    
    # TSV íŒŒì¼ ì½ê¸°
    df = pd.read_csv(gsk3b_data_path, sep='\t')
    print(f"ì›ë³¸ ë°ì´í„°: {len(df)} rows")
    
    # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ ë° ë§¤í•‘
    column_mapping = {
        'Ligand SMILES': 'Standardized_SMILES',
        'BindingDB Target Chain Sequence': 'Target_Sequence',
        'Standard Type': 'Standard Type',
        'pChEMBL': 'pChEMBL_Value'
    }
    
    # ì»¬ëŸ¼ ì´ë¦„ ë³€ê²½
    for old_col, new_col in column_mapping.items():
        if old_col in df.columns:
            df[new_col] = df[old_col]
    
    # ì§€ì›í•˜ëŠ” affinity íƒ€ì…ë§Œ í•„í„°ë§
    df_filtered = df[df['Standard Type'].isin(AFFINITY_TYPES)].copy()
    print(f"ì§€ì›í•˜ëŠ” affinity íƒ€ì… í•„í„°ë§ í›„: {len(df_filtered)} rows")
    
    # Affinity íƒ€ì…ë³„ ë¶„í¬ í™•ì¸
    affinity_counts = df_filtered['Standard Type'].value_counts()
    print("Affinity íƒ€ì… ë¶„í¬:")
    for aff_type, count in affinity_counts.items():
        print(f"  {aff_type}: {count}ê°œ")
    
    # ê²°ì¸¡ê°’ ì œê±°
    df_clean = df_filtered.dropna(subset=['Standardized_SMILES', 'Target_Sequence', 'pChEMBL_Value']).reset_index(drop=True)
    print(f"ê²°ì¸¡ê°’ ì œê±° í›„: {len(df_clean)} rows")
    
    # ---------------------------
    # 2. ë°ì´í„° ë¶„í•  (ì‹¤ìŠµë…¸íŠ¸: ì‘ì€ ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹ ì¤‘í•œ ë¶„í• )
    # ---------------------------
    print(f"\n[STEP 2] ë°ì´í„° ë¶„í• ")
    
    # Stratified split (affinity type ê¸°ì¤€)
    from sklearn.model_selection import train_test_split
    
    # ë¨¼ì € train+valê³¼ test ë¶„í• 
    train_val_df, test_df = train_test_split(
        df_clean, 
        test_size=test_split, 
        random_state=SEED,
        stratify=df_clean['Standard Type']
    )
    
    # trainê³¼ validation ë¶„í• 
    train_df, val_df = train_test_split(
        train_val_df,
        test_size=validation_split/(1-test_split),
        random_state=SEED,
        stratify=train_val_df['Standard Type']
    )
    
    print(f"Train: {len(train_df)} samples")
    print(f"Validation: {len(val_df)} samples")
    print(f"Test: {len(test_df)} samples")
    
    # ë°ì´í„°ì…‹ ìƒì„±
    train_dataset = GSK3BDataset(train_df, augment_data=data_augmentation)
    val_dataset = GSK3BDataset(val_df, augment_data=False)
    test_dataset = GSK3BDataset(test_df, augment_data=False)
    
    # ë°ì´í„° ë¡œë” ìƒì„±
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, 
                             collate_fn=collate_fn_multi, num_workers=2)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,
                           collate_fn=collate_fn_multi, num_workers=2)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,
                            collate_fn=collate_fn_multi, num_workers=2)
    
    # ---------------------------
    # 3. Pre-trained ëª¨ë¸ ë¡œë”©
    # ---------------------------
    print(f"\n[STEP 3] Pre-trained ëª¨ë¸ ë¡œë”©")
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model = MultiAffinityNet(d_model=256, d_hidden=256, n_layers=2, dropout=0.1)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # Pre-trained weights ë¡œë”©
    checkpoint = torch.load(pretrained_model_path, map_location=device)
    if 'model' in checkpoint:
        model.load_state_dict(checkpoint['model'])
        print("Pre-trained model state dict ë¡œë”© ì™„ë£Œ")
    else:
        model.load_state_dict(checkpoint)
        print("Pre-trained model weights ë¡œë”© ì™„ë£Œ")
    
    model.to(device)
    
    # ---------------------------
    # 4. Fine-tuning ì„¤ì •
    # ---------------------------
    print(f"\n[STEP 4] Fine-tuning ì„¤ì •")
    
    # ì‹¤ìŠµë…¸íŠ¸: ì¸ì½”ë” ë™ê²° ì˜µì…˜
    if freeze_encoder:
        print("ì¸ì½”ë” ë¶€ë¶„ ë™ê²°")
        for name, param in model.named_parameters():
            if any(layer in name for layer in ['emb_smi', 'emb_seq', 'conv_smi', 'conv_seq']):
                param.requires_grad = False
    
    # í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total_params = sum(p.numel() for p in model.parameters())
    print(f"ì „ì²´ íŒŒë¼ë¯¸í„°: {total_params:,}")
    print(f"í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„°: {trainable_params:,} ({trainable_params/total_params*100:.1f}%)")
    
    # ì˜µí‹°ë§ˆì´ì € ì„¤ì •
    optimizer = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=lr,
        weight_decay=weight_decay
    )

    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • (verbose ë§¤ê°œë³€ìˆ˜ ì œê±°)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=5
    )
    
    # AMP ìŠ¤ì¼€ì¼ëŸ¬
    scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())
    
    # Early stopping
    early_stopper = EarlyStopper(patience=early_stopping_patience, min_delta=1e-4)
    
    # ---------------------------
    # 5. Fine-tuning í•™ìŠµ
    # ---------------------------
    print(f"\n[STEP 5] Fine-tuning í•™ìŠµ ì‹œì‘")
    print(f"Epochs: {epochs}, Learning Rate: {lr}, Batch Size: {batch_size}")
    print("-" * 60)
    
    best_val_rmse = float('inf')
    best_model_path = os.path.join(output_dir, "best_gsk3b_model.pt")
    history = []
    
    for epoch in range(1, epochs + 1):
        # í•™ìŠµ
        train_loss, train_metrics, train_by_type = run_epoch_multi_amp(
            model, train_loader, optimizer=optimizer, scaler=scaler, 
            device=device, scheduler=None, grad_accum=1
        )
        
        # ê²€ì¦
        val_loss, val_metrics, val_by_type = run_epoch_multi_amp(
            model, val_loader, optimizer=None, scaler=None, device=device
        )
        
        # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸ (verbose ì¶œë ¥ì„ ìˆ˜ë™ìœ¼ë¡œ ì²˜ë¦¬)
        old_lr = optimizer.param_groups[0]['lr']
        scheduler.step(val_loss)
        new_lr = optimizer.param_groups[0]['lr']
        if new_lr != old_lr:
            print(f"    Learning rate reduced: {old_lr:.6f} â†’ {new_lr:.6f}")
        
        # ì§„í–‰ìƒí™© ì¶œë ¥
        print(f"Epoch {epoch:3d}/{epochs} | "
              f"Train Loss: {train_loss:.4f} RMSE: {train_metrics['RMSE']:.3f} | "
              f"Val Loss: {val_loss:.4f} RMSE: {val_metrics['RMSE']:.3f} RÂ²: {val_metrics['R2']:.3f}")
        
        # Affinityë³„ ì„±ëŠ¥ ì¶œë ¥ (ë§¤ 10 ì—í¬í¬ë§ˆë‹¤)
        if epoch % 10 == 0:
            print("  Validation by affinity type:")
            for aff_type in AFFINITY_TYPES:
                if val_by_type[aff_type]['count'] > 0:
                    metrics = val_by_type[aff_type]
                    print(f"    {aff_type}: RMSE={metrics['RMSE']:.3f}, RÂ²={metrics['R2']:.3f} (n={metrics['count']})")
        
        # Best model ì €ì¥
        if val_metrics['RMSE'] < best_val_rmse:
            best_val_rmse = val_metrics['RMSE']
            torch.save({
                'model': model.state_dict(),
                'optimizer': optimizer.state_dict(),
                'scheduler': scheduler.state_dict(),
                'epoch': epoch,
                'best_val_rmse': best_val_rmse,
                'train_metrics': train_metrics,
                'val_metrics': val_metrics,
                'val_by_type': val_by_type,
                'fine_tuning_config': {
                    'lr': lr,
                    'batch_size': batch_size,
                    'freeze_encoder': freeze_encoder,
                    'data_augmentation': data_augmentation
                }
            }, best_model_path)
            print(f"    â†’ Best model saved (RMSE: {best_val_rmse:.3f})")
        
        # íˆìŠ¤í† ë¦¬ ì €ì¥
        history.append({
            'epoch': epoch,
            'train_loss': train_loss,
            'train_rmse': train_metrics['RMSE'],
            'train_r2': train_metrics['R2'],
            'val_loss': val_loss,
            'val_rmse': val_metrics['RMSE'],
            'val_r2': val_metrics['R2'],
            'lr': optimizer.param_groups[0]['lr']
        })
        
        # Early stopping ì²´í¬
        if early_stopper.step(val_metrics['RMSE']):
            print(f"\nEarly stopping at epoch {epoch}")
            break
    
    # ---------------------------
    # 6. ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€
    # ---------------------------
    print(f"\n[STEP 6] ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€")
    
    # Best model ë¡œë”©
    checkpoint = torch.load(best_model_path, map_location=device)
    model.load_state_dict(checkpoint['model'])
    
    # í…ŒìŠ¤íŠ¸ í‰ê°€
    test_loss, test_metrics, test_by_type = run_epoch_multi_amp(
        model, test_loader, optimizer=None, scaler=None, device=device
    )
    
    print(f"\n=== GSK3Î² Fine-tuning Results ===")
    print(f"Overall Test Performance:")
    print(f"  RMSE: {test_metrics['RMSE']:.3f}")
    print(f"  MAE:  {test_metrics['MAE']:.3f}")
    print(f"  RÂ²:   {test_metrics['R2']:.3f}")
    
    print(f"\nPer-Affinity Performance:")
    for aff_type in AFFINITY_TYPES:
        if test_by_type[aff_type]['count'] > 0:
            metrics = test_by_type[aff_type]
            print(f"  {aff_type:>4}: RMSE={metrics['RMSE']:.3f}, RÂ²={metrics['R2']:.3f} (n={metrics['count']})")
    
    # ---------------------------
    # 7. ê²°ê³¼ ì €ì¥
    # ---------------------------
    results = {
        'test_metrics': test_metrics,
        'test_by_type': test_by_type,
        'best_val_rmse': best_val_rmse,
        'training_history': history,
        'fine_tuning_config': {
            'pretrained_model_path': pretrained_model_path,
            'epochs': epoch,  # ì‹¤ì œ ì‹¤í–‰ëœ ì—í¬í¬
            'lr': lr,
            'batch_size': batch_size,
            'freeze_encoder': freeze_encoder,
            'data_augmentation': data_augmentation,
            'early_stopping_patience': early_stopping_patience
        },
        'dataset_info': {
            'total_samples': len(df_clean),
            'train_samples': len(train_df),
            'val_samples': len(val_df),
            'test_samples': len(test_df),
            'affinity_distribution': affinity_counts.to_dict()
        }
    }
    
    # ê²°ê³¼ ì €ì¥
    results_path = os.path.join(output_dir, "gsk3b_finetuning_results.json")
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    # ë°ì´í„° ë¶„í•  ì €ì¥
    train_df.to_csv(os.path.join(output_dir, "train_split.tsv"), sep='\t', index=False)
    val_df.to_csv(os.path.join(output_dir, "val_split.tsv"), sep='\t', index=False)
    test_df.to_csv(os.path.join(output_dir, "test_split.tsv"), sep='\t', index=False)
    
    print(f"\nëª¨ë“  ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_dir}")
    print(f"  - Best model: {best_model_path}")
    print(f"  - Results: {results_path}")
    
    return model, results

# ---------------------------
# 5-fold Fine-tuning í•¨ìˆ˜ ì¶”ê°€
# ---------------------------
def fine_tune_gsk3b_5fold(pretrained_model_path,
                          gsk3b_data_path,
                          output_dir="./fientuning_results",
                          epochs=50,
                          lr=1e-4,
                          weight_decay=1e-4,
                          batch_size=16,
                          early_stopping_patience=10,
                          test_split=0.1,
                          freeze_encoder=False,
                          data_augmentation=True):
    """
    5-fold cross-validationì„ ì‚¬ìš©í•œ GSK3Î² Fine-tuning
    """
    from sklearn.model_selection import StratifiedKFold
    
    print("=" * 80)
    print("GSK3Î² 5-Fold Cross-Validation Fine-tuning")
    print("=" * 80)
    
    os.makedirs(output_dir, exist_ok=True)
    
    # ë°ì´í„° ë¡œë”©
    df = pd.read_csv(gsk3b_data_path, sep='\t')
    
    # ì»¬ëŸ¼ ë§¤í•‘
    column_mapping = {
        'Ligand SMILES': 'Standardized_SMILES',
        'BindingDB Target Chain Sequence': 'Target_Sequence',
        'Standard Type': 'Standard Type',
        'pChEMBL': 'pChEMBL_Value'
    }
    
    for old_col, new_col in column_mapping.items():
        if old_col in df.columns:
            df[new_col] = df[old_col]
    
    # í•„í„°ë§ ë° ì •ë¦¬
    df_filtered = df[df['Standard Type'].isin(AFFINITY_TYPES)].copy()
    df_clean = df_filtered.dropna(subset=['Standardized_SMILES', 'Target_Sequence', 'pChEMBL_Value']).reset_index(drop=True)
    
    print(f"Total samples: {len(df_clean)}")
    
    # Test set ë¶„ë¦¬
    from sklearn.model_selection import train_test_split
    train_val_df, test_df = train_test_split(
        df_clean, 
        test_size=test_split, 
        random_state=SEED,
        stratify=df_clean['Standard Type']
    )
    
    print(f"Train+Val: {len(train_val_df)}, Test: {len(test_df)}")
    
    # 5-fold CV ì„¤ì •
    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)
    
    fold_results = []
    best_models = []
    
    for fold, (train_idx, val_idx) in enumerate(kfold.split(train_val_df, train_val_df['Standard Type'])):
        print(f"\n{'='*20} Fold {fold+1}/5 {'='*20}")
        
        # í´ë“œë³„ ë°ì´í„° ë¶„í• 
        fold_train_df = train_val_df.iloc[train_idx].reset_index(drop=True)
        fold_val_df = train_val_df.iloc[val_idx].reset_index(drop=True)
        
        print(f"Fold {fold+1} - Train: {len(fold_train_df)}, Val: {len(fold_val_df)}")
        
        # ë°ì´í„°ì…‹ ìƒì„±
        train_dataset = GSK3BDataset(fold_train_df, augment_data=data_augmentation)
        val_dataset = GSK3BDataset(fold_val_df, augment_data=False)
        
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, 
                                 collate_fn=collate_fn_multi, num_workers=2)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,
                               collate_fn=collate_fn_multi, num_workers=2)
        
        # ëª¨ë¸ ì´ˆê¸°í™” ë° Pre-trained weights ë¡œë”© (í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
        model = MultiAffinityNet(d_model=256, d_hidden=256, n_layers=2, dropout=0.1)
        device = "cuda" if torch.cuda.is_available() else "cpu"
        
        checkpoint = torch.load(pretrained_model_path, map_location=device)
        
        if 'model' in checkpoint:
            pretrained_dict = checkpoint['model']
        else:
            pretrained_dict = checkpoint
        
        # í˜„ì¬ ëª¨ë¸ê³¼ í˜¸í™˜ë˜ëŠ” íŒŒë¼ë¯¸í„°ë§Œ ë¡œë”©
        model_dict = model.state_dict()
        filtered_dict = {}
        skipped_params = []
        
        for k, v in pretrained_dict.items():
            if k in model_dict and model_dict[k].shape == v.shape:
                filtered_dict[k] = v
            else:
                skipped_params.append(k)
        
        # í˜¸í™˜ë˜ëŠ” íŒŒë¼ë¯¸í„°ë§Œ ì—…ë°ì´íŠ¸
        model_dict.update(filtered_dict)
        model.load_state_dict(model_dict)
        
        print(f"Pre-trained model ë¡œë”© ì™„ë£Œ")
        print(f"  ë¡œë”©ëœ íŒŒë¼ë¯¸í„°: {len(filtered_dict)}/{len(pretrained_dict)}")
        if skipped_params:
            print(f"  ê±´ë„ˆë›´ íŒŒë¼ë¯¸í„°: {skipped_params}")
        
        model.to(device)
        
        # ì¸ì½”ë” ë™ê²° ì„¤ì •
        if freeze_encoder:
            for name, param in model.named_parameters():
                if any(layer in name for layer in ['emb_smi', 'emb_seq', 'conv_smi', 'conv_seq']):
                    param.requires_grad = False
        
        # ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¤„ëŸ¬
        optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=lr,
            weight_decay=weight_decay
        )
        
        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, mode='min', factor=0.5, patience=5
        )
        
        scaler = torch.cuda.amp.GradScaler(enabled=torch.cuda.is_available())
        early_stopper = EarlyStopper(patience=early_stopping_patience, min_delta=1e-4)
        
        # í•™ìŠµ
        best_val_rmse = float('inf')
        fold_model_path = os.path.join(output_dir, f"best_gsk3b_model_fold{fold}.pt")
        
        for epoch in range(1, epochs + 1):
            # í•™ìŠµ
            train_loss, train_metrics, _ = run_epoch_multi_amp(
                model, train_loader, optimizer=optimizer, scaler=scaler, 
                device=device, scheduler=None, grad_accum=1
            )
            
            # ê²€ì¦
            val_loss, val_metrics, val_by_type = run_epoch_multi_amp(
                model, val_loader, optimizer=None, scaler=None, device=device
            )
            
            scheduler.step(val_loss)
            
            # ì§„í–‰ìƒí™© ì¶œë ¥ (ë§¤ 10 ì—í¬í¬)
            if epoch % 10 == 0:
                print(f"  Epoch {epoch:3d}/{epochs} | Val RMSE: {val_metrics['RMSE']:.3f} RÂ²: {val_metrics['R2']:.3f}")
            
            # Best model ì €ì¥
            if val_metrics['RMSE'] < best_val_rmse:
                best_val_rmse = val_metrics['RMSE']
                torch.save({
                    'model': model.state_dict(),
                    'fold': fold,
                    'epoch': epoch,
                    'val_rmse': best_val_rmse,
                    'val_metrics': val_metrics,
                    'val_by_type': val_by_type
                }, fold_model_path)
            
            # Early stopping
            if early_stopper.step(val_metrics['RMSE']):
                print(f"  Early stopping at epoch {epoch}")
                break
        
        # í´ë“œ ê²°ê³¼ ì €ì¥
        fold_results.append({
            'fold': fold,
            'best_val_rmse': best_val_rmse,
            'val_metrics': val_metrics,
            'val_by_type': val_by_type,
            'model_path': fold_model_path
        })
        
        best_models.append(fold_model_path)
        
        print(f"Fold {fold+1} completed - Best Val RMSE: {best_val_rmse:.3f}")
    
    # ì „ì²´ í´ë“œ ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*20} 5-Fold Results Summary {'='*20}")
    
    fold_rmses = [result['best_val_rmse'] for result in fold_results]
    print(f"Fold RMSEs: {[f'{rmse:.3f}' for rmse in fold_rmses]}")
    print(f"Mean RMSE: {np.mean(fold_rmses):.3f} Â± {np.std(fold_rmses):.3f}")
    
    # í…ŒìŠ¤íŠ¸ ì…‹ì—ì„œ ì•™ìƒë¸” í‰ê°€
    print(f"\n{'='*20} Ensemble Test Evaluation {'='*20}")
    
    test_dataset = GSK3BDataset(test_df, augment_data=False)
    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,
                           collate_fn=collate_fn_multi, num_workers=2)
    
    # ì•™ìƒë¸” ì˜ˆì¸¡ (5ê°œ ëª¨ë¸ í‰ê· )
    device = "cuda" if torch.cuda.is_available() else "cpu"
    all_predictions = []
    all_targets = []
    
    for batch in test_loader:
        X_smi, L_smi = batch['X_smi'].to(device), batch['L_smi'].to(device)
        X_seq, L_seq = batch['X_seq'].to(device), batch['L_seq'].to(device)
        affinity_types = batch['affinity_types'].to(device)
        targets = batch['y'].cpu().numpy()
        
        batch_predictions = []
        
        # ê° í´ë“œ ëª¨ë¸ë¡œ ì˜ˆì¸¡
        for model_path in best_models:
            model = MultiAffinityNet(d_model=256, d_hidden=256, n_layers=2, dropout=0.1)
            checkpoint = torch.load(model_path, map_location=device)
            model.load_state_dict(checkpoint['model'])
            model.to(device)
            model.eval()
            
            with torch.no_grad():
                pred = model(X_smi, L_smi, X_seq, L_seq, affinity_types)
                batch_predictions.append(pred.cpu().numpy())
        
        # ì•™ìƒë¸” í‰ê· 
        ensemble_pred = np.mean(batch_predictions, axis=0)
        all_predictions.append(ensemble_pred)
        all_targets.append(targets)
    
    # ìµœì¢… ì•™ìƒë¸” ì„±ëŠ¥ ê³„ì‚°
    final_predictions = np.concatenate(all_predictions)
    final_targets = np.concatenate(all_targets)
    
    ensemble_metrics = compute_metrics(final_targets, final_predictions)
    
    print(f"Ensemble Test Performance:")
    print(f"  RMSE: {ensemble_metrics['RMSE']:.3f}")
    print(f"  MAE:  {ensemble_metrics['MAE']:.3f}")
    print(f"  RÂ²:   {ensemble_metrics['R2']:.3f}")
    
    # ê²°ê³¼ ì €ì¥
    ensemble_results = {
        'fold_results': fold_results,
        'ensemble_test_metrics': ensemble_metrics,
        'fold_rmses': fold_rmses,
        'mean_rmse': float(np.mean(fold_rmses)),
        'std_rmse': float(np.std(fold_rmses)),
        'model_paths': best_models,
        'config': {
            'epochs': epochs,
            'lr': lr,
            'batch_size': batch_size,
            'freeze_encoder': freeze_encoder,
            'data_augmentation': data_augmentation
        }
    }
    
    # ê²°ê³¼ íŒŒì¼ ì €ì¥
    with open(os.path.join(output_dir, "5fold_results.json"), 'w') as f:
        json.dump(ensemble_results, f, indent=2)
    
    # í…ŒìŠ¤íŠ¸ ë°ì´í„° ì €ì¥
    test_df.to_csv(os.path.join(output_dir, "test_split.tsv"), sep='\t', index=False)
    
    print(f"\n5-fold ê²°ê³¼ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤: {output_dir}")
    print(f"ëª¨ë¸ íŒŒì¼ë“¤: {best_models}")
    
    return best_models, ensemble_results

# ---------------------------
# ì‹¤í–‰ í•¨ìˆ˜
# ---------------------------
if __name__ == "__main__":
    # Get project root directory (LAIDD/)
    PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

    # ê²½ë¡œ ì„¤ì • (í˜„ì¬ ë””ë ‰í† ë¦¬ ê¸°ì¤€)
    PRETRAINED_MODEL_PATH = os.path.join(PROJECT_ROOT, "results", "pretraining", "best_model.pt")
    GSK3B_DATA_PATH = os.path.join(PROJECT_ROOT, "data", "finetuning_data.tsv")
    OUTPUT_DIR = os.path.join(PROJECT_ROOT, "results", "finetuning")
    
    print("GSK3Î² Fine-tuning ì‹œì‘")
    print(f"Pre-trained model: {PRETRAINED_MODEL_PATH}")
    print(f"GSK3Î² data: {GSK3B_DATA_PATH}")
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(PRETRAINED_MODEL_PATH):
        print(f"ERROR: Pre-trained model not found: {PRETRAINED_MODEL_PATH}")
        sys.exit(1)
    
    if not os.path.exists(GSK3B_DATA_PATH):
        print(f"ERROR: GSK3B data not found: {GSK3B_DATA_PATH}")
        sys.exit(1)
    
    # ì‚¬ìš©ìì—ê²Œ ì„ íƒ ì˜µì…˜ ì œê³µ
    print("\nFine-tuning ë°©ë²•ì„ ì„ íƒí•˜ì„¸ìš”:")
    print("1. Single model fine-tuning")
    print("2. 5-fold cross-validation fine-tuning")
    
    choice = input("ì„ íƒ (1 ë˜ëŠ” 2): ").strip()
    
    try:
        if choice == "1":
            # ë‹¨ì¼ ëª¨ë¸ íŒŒì¸íŠœë‹
            model, results = fine_tune_gsk3b(
                pretrained_model_path=PRETRAINED_MODEL_PATH,
                gsk3b_data_path=GSK3B_DATA_PATH,
                output_dir=OUTPUT_DIR,
                epochs=50,
                lr=1e-4,  # ì‹¤ìŠµë…¸íŠ¸: ë‚®ì€ í•™ìŠµë¥ 
                weight_decay=1e-4,
                batch_size=16,
                early_stopping_patience=15,
                validation_split=0.2,
                test_split=0.1,
                freeze_encoder=False,  # ëª¨ë“  ë ˆì´ì–´ í•™ìŠµ
                data_augmentation=True  # ê³¼ì í•© ë°©ì§€
            )
            
            print("\nğŸ‰ GSK3Î² Fine-tuningì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            
        elif choice == "2":
            # 5-fold íŒŒì¸íŠœë‹
            FOLD_OUTPUT_DIR = "./gsk3b_5fold_results"
            
            model_paths, ensemble_results = fine_tune_gsk3b_5fold(
                pretrained_model_path=PRETRAINED_MODEL_PATH,
                gsk3b_data_path=GSK3B_DATA_PATH,
                output_dir=FOLD_OUTPUT_DIR,
                epochs=50,
                lr=1e-4,
                weight_decay=1e-4,
                batch_size=16,
                early_stopping_patience=10,
                test_split=0.1,
                freeze_encoder=False,
                data_augmentation=True
            )
            
            print("\nğŸ‰ GSK3Î² 5-fold Fine-tuningì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
            print(f"ì•™ìƒë¸” ëª¨ë¸ë“¤: {len(model_paths)}ê°œ")
            print(f"ì•™ìƒë¸” í…ŒìŠ¤íŠ¸ RMSE: {ensemble_results['ensemble_test_metrics']['RMSE']:.3f}")
            
        else:
            print("ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1 ë˜ëŠ” 2ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            sys.exit(1)
            
    except Exception as e:
        print(f"\nâŒ Fine-tuning ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()